import torch
import evaluate
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed, pipeline
from peft import PeftModel
from datasets import load_dataset
import os

compute_dtype = torch.float16
device_map = {"": 0}
set_seed(1234)

# 基础模型路径
base_model_path = "microsoft/Phi-3-mini-4k-instruct"

# 加载 tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    base_model_path, 
    trust_remote_code=True, 
    add_eos_token=True, 
    use_fast=True
)
tokenizer.pad_token = tokenizer.unk_token
tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
tokenizer.padding_side = 'left'

# Load, filter and seperate dataset
dataset = load_dataset(dataset_name, split="train")
dataset = dataset.filter(lambda example: example['cluster'] == cli_args.cluster_id)  # cluster_id

# 目前只用 30% 数据（记得取消！！！）
subset_size = int(len(dataset) * 0.3)
dataset = dataset.select(range(subset_size))

# 数据集准备
dataset_name = "lmong/clustered_oss_dataset"
dataset = load_dataset(dataset_name, split="train")
dataset = dataset.filter(lambda example: example['cluster'] == 0)
subset_size = int(len(dataset) * 0.2)
dataset = dataset.select(range(subset_size))

def create_message_column(row):
    return {"messages": [{"content": f"{row['instruction']}\n Input: {row['prompt']}", "role": "user"},
                         {"content": row['response'], "role": "assistant"}]}

def format_dataset_chatml(row):
    return {"text": tokenizer.apply_chat_template(row["messages"], add_generation_prompt=False, tokenize=False)}

chatml_datasets = dataset.map(create_message_column).map(format_dataset_chatml).train_test_split(test_size=0.2)

# 加载 ROUGE 评估器
rouge_metric = evaluate.load("rouge")

def test_inference(pipe, prompt):
    prompt = pipe.tokenizer.apply_chat_template([{"role": "user", "content": prompt}], tokenize=False, add_generation_prompt=True)
    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95, max_time=180)
    return outputs[0]['generated_text'][len(prompt):].strip()

def calculate_rouge(pipe, row):
    response = test_inference(pipe, row['messages'][0]['content'])
    result = rouge_metric.compute(predictions=[response], references=[row['response']], use_stemmer=True)
    result = {key: value * 100 for key, value in result.items()}
    result['response'] = response
    return result



# 遍历所有 adapters 进行融合与评估
adapter_path_template = "./dxltt1211/task0_adapter{}"
for i in range(10):
    print(f"Processing adapter {i}...")
    
    # 加载基础模型
    model = AutoModelForCausalLM.from_pretrained(base_model_path, trust_remote_code=True, torch_dtype=compute_dtype)
    
    # 融合对应的 adapter
    adapter_path = adapter_path_template.format(i)
    model = PeftModel.from_pretrained(model, adapter_path)
    model = model.merge_and_unload()
    
    # 创建生成管道
    pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device=0)
    pipe.tokenizer.apply_chat_template([{"role": "user", "content": chatml_datasets['test'][0]['messages'][0]['content']}], tokenize=False, add_generation_prompt=True)
    # 计算 ROUGE 分数
    metrics = chatml_datasets['test'].select(range(0, 102)).map(lambda row: calculate_rouge(pipe, row), batched=False)

    # 打印并保存结果
    rouge1_mean = np.mean(metrics['rouge1'])
    rouge2_mean = np.mean(metrics['rouge2'])
    rougel_mean = np.mean(metrics['rougeL'])
    rougelsum_mean = np.mean(metrics['rougeLsum'])

    print(f"Adapter {i} - Rouge 1 Mean: {rouge1_mean}")
    print(f"Adapter {i} - Rouge 2 Mean: {rouge2_mean}")
    print(f"Adapter {i} - Rouge L Mean: {rougel_mean}")
    print(f"Adapter {i} - Rouge Lsum Mean: {rougelsum_mean}")

    # 保存结果到文件
    output_file = f"rouge_phi3_task0_adapter{i}.txt"
    with open(output_file, "w") as f:
        f.write(f"Adapter {i} - Rouge 1 Mean: {rouge1_mean}\n")
        f.write(f"Adapter {i} - Rouge 2 Mean: {rouge2_mean}\n")
        f.write(f"Adapter {i} - Rouge L Mean: {rougel_mean}\n")
        f.write(f"Adapter {i} - Rouge Lsum Mean: {rougelsum_mean}\n")

    print(f"Results saved to {output_file}")




# 计算平均融合后的 adapter 的 ROUGE 分数
print("Processing avg10adapters_task0_phi-3...")
model = AutoModelForCausalLM.from_pretrained(base_model_path, trust_remote_code=True, torch_dtype=compute_dtype)
model = PeftModel.from_pretrained(model, "dxltt1211/avg10adapters_task0_phi-3")
model = model.merge_and_unload()

# 创建生成管道
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device=0)
pipe.tokenizer.apply_chat_template([{"role": "user", "content": chatml_datasets['test'][0]['messages'][0]['content']}], tokenize=False, add_generation_prompt=True)
# 计算 ROUGE 分数
metrics = chatml_datasets['test'].select(range(0, 102)).map(lambda row: calculate_rouge(pipe, row), batched=False)

# 打印并保存结果
rouge1_mean = np.mean(metrics['rouge1'])
rouge2_mean = np.mean(metrics['rouge2'])
rougel_mean = np.mean(metrics['rougeL'])
rougelsum_mean = np.mean(metrics['rougeLsum'])

print(f"Avg Adapter - Rouge 1 Mean: {rouge1_mean}")
print(f"Avg Adapter - Rouge 2 Mean: {rouge2_mean}")
print(f"Avg Adapter - Rouge L Mean: {rougel_mean}")
print(f"Avg Adapter - Rouge Lsum Mean: {rougelsum_mean}")

# 保存结果到文件
output_file = "rouge_phi3_task0_avg10adapters.txt"
with open(output_file, "w") as f:
    f.write(f"Avg Adapter - Rouge 1 Mean: {rouge1_mean}\n")
    f.write(f"Avg Adapter - Rouge 2 Mean: {rouge2_mean}\n")
    f.write(f"Avg Adapter - Rouge L Mean: {rougel_mean}\n")
    f.write(f"Avg Adapter - Rouge Lsum Mean: {rougelsum_mean}\n")

print(f"Results saved to {output_file}")





# import torch
# import evaluate
# import numpy as np
# from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed, pipeline
# from peft import PeftModel
# from datasets import load_dataset
# import os
# compute_dtype = torch.float16
# set_seed(1234)


# base_model_path = "microsoft/Phi-3-mini-4k-instruct"
# model = AutoModelForCausalLM.from_pretrained(base_model_path, trust_remote_code=True, torch_dtype=compute_dtype)
# model = PeftModel.from_pretrained(model, "dxltt1211/avg10adapters_task0_phi-3")
# model = model.merge_and_unload()
# tokenizer = AutoTokenizer.from_pretrained(
#     base_model_path, 
#     trust_remote_code=True, 
#     add_eos_token=True, 
#     use_fast=True
# )
# tokenizer.pad_token = tokenizer.unk_token
# tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
# tokenizer.padding_side = 'left'


# # 数据集准备(合并代码后可去掉)
# dataset_name = "lmong/clustered_oss_dataset"
# dataset = load_dataset(dataset_name, split="train")
# dataset = dataset.filter(lambda example: example['cluster'] == 0)
# subset_size = int(len(dataset) * 0.2)  
# dataset = dataset.select(range(subset_size)) 
# def create_message_column(row):
#     return {"messages": [{"content": f"{row['instruction']}\n Input: {row['prompt']}", "role": "user"},
#                          {"content": row['response'], "role": "assistant"}]}
# def format_dataset_chatml(row):
#     return {"text": tokenizer.apply_chat_template(row["messages"], add_generation_prompt=False, tokenize=False)}
    
# pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
# pipe.tokenizer.apply_chat_template([{"role": "user", "content": chatml_datasets['test'][0]['messages'][0]['content']}], tokenize=False, add_generation_prompt=True)

# def test_inference(prompt):
#     prompt = pipe.tokenizer.apply_chat_template([{"role": "user", "content": prompt}], tokenize=False, add_generation_prompt=True)
#     outputs = pipe(prompt, max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95, max_time= 180)
#     return outputs[0]['generated_text'][len(prompt):].strip()

# chatml_datasets = dataset.map(create_message_column).map(format_dataset_chatml).train_test_split(test_size=0.2)
# metricas = chatml_datasets['test'].select(range(0, 102)).map(calculate_rogue, batched=False)


# rouge_metric = evaluate.load("rouge")
# def calculate_rogue(row):
#     response = test_inference(row['messages'][0]['content'])
#     result = rouge_metric.compute(predictions=[response], references=[row['response']], use_stemmer=True)
#     #result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
#     result = {key: value * 100 for key, value in result.items()}
#     result['response'] = response
#     return result

# print("Rouge 1 Mean: ",np.mean(metricas['rouge1']))
# print("Rouge 2 Mean: ",np.mean(metricas['rouge2']))
# print("Rouge L Mean: ",np.mean(metricas['rougeL']))
# print("Rouge Lsum Mean: ",np.mean(metricas['rougeLsum']))
# # Define the file path to save the results
# output_file = "rouge_phi3_task0_10adapters.txt"

# # Open the file in write mode and write the results
# with open(output_file, "w") as f:
#     f.write("Rouge 1 Mean: " + str(np.mean(metricas['rouge1'])) + "\n")
#     f.write("Rouge 2 Mean: " + str(np.mean(metricas['rouge2'])) + "\n")
#     f.write("Rouge L Mean: " + str(np.mean(metricas['rougeL'])) + "\n")
#     f.write("Rouge Lsum Mean: " + str(np.mean(metricas['rougeLsum'])) + "\n")

# print(f"Results saved to {output_file}")

